---
title: "Lab 4"
author: "Shoara Chowdhury"
date: "11:59PM March 10, 2021"
output:
  pdf_document: default
  word_document: default
---

Load up the famous iris dataset. We are going to do a different prediction problem. Imagine the only input x is Species and you are trying to predict y which is Petal.Length. A reasonable prediction is the average petal length within each Species. Prove that this is the OLS model by fitting an appropriate `lm` and then using the predict function to verify.

```{r}
data(iris)
mod = lm(Petal.Length ~ Species, iris)
mean(iris$Petal.Length[iris$Species=="setosa"])
mean(iris$Petal.Length[iris$Species=="versicolor"])
mean(iris$Petal.Length[iris$Species=="virginica"])

predict(mod,data.frame(Species =("setosa"))) 
predict(mod,data.frame(Species =("versicolor"))) 
predict(mod,data.frame(Species =("virginica"))) 



```

Construct the design matrix with an intercept, X without using `model.matrix`.

```{r}
x <- cbind(1,iris$Species == "versicolor", iris$Species == "virginica" )

head(x)
```

Find the hat matrix H for this regression.

```{r}
H = x %*% solve(t(x) %*% x) %*% t(x)
Matrix:: rankMatrix(H)
```

Verify this hat matrix is symmetric using the `expect_equal` function in the package `testthat`.

```{r}
pacman::p_load(testthat)
expect_equal(H, t(H))
```

Verify this hat matrix is idempotent using the `expect_equal` function in the package `testthat`.

```{r}
expect_equal(H,H%*%H)
```

Using the `diag` function, find the trace of the hat matrix.

```{r}
diag(H)
```

It turns out the trace of a hat matrix is the same as its rank! But we don't have time to prove these interesting and useful facts..

For masters students: create a matrix X-perpendicular.

```{r}
sum(diag(H))
```

Using the hat matrix, compute the yhat vector and using the projection onto the residual space, compute the e vector and verify they are orthogonal to each other.

```{r}
y = iris$Petal.Length
y_hat = H%*% y
e = (diag(nrow(iris))-H) %*% y
e
```

Compute SST, SSR and SSE and R^2 and then show that SST = SSR + SSE.

```{r}
SSE= t(e) %*% e 
y_bar = mean(y)
SST= t(y-y_bar)%*% (y-y_bar)
Rsq = 1- SSE/SST 
Rsq
SSR= t(y-y_bar)%*% (y-y_bar)
SSR 
#expect_equal(SSR+SSE, SST)
var(y)
var(e)
```

Find the angle theta between y - ybar 1 and yhat - ybar 1 and then verify that its cosine squared is the same as the R^2 from the previous problem.

```{r}
theta = acos(t(y-y_bar) %*% (y_hat-y_bar) / sqrt(SST * SSR))
theta = (180 / pi)
```

Project the y vector onto each column of the X matrix and test if the sum of these projections is the same as yhat.

```{r}
prjt1 = ( x[,1] %*% t(x[,1]) / as.numeric( t(x[,1]) %*% x[,1]) ) %*% y
prjt2 = ( x[,2] %*% t(x[,2]) / as.numeric( t(x[,2]) %*% x[,2]) ) %*% y
prjt3 = ( x[,3] %*% t(x[,3]) / as.numeric( t(x[,3]) %*% x[,3]) ) %*% y
```

Construct the design matrix without an intercept, X, without using `model.matrix`.

```{r}
Mi <- cbind(0,iris$Species == "versicolor", iris$Species == "virginica" )
```

Find the OLS estimates using this design matrix. It should be the sample averages of the petal lengths within species.

```{r}
#expect_equal(Mi,Mi%*%Mi)
```

Verify the hat matrix constructed from this design matrix is the same as the hat matrix constructed from the design matrix with the intercept. (Fact: orthogonal projection matrices are unique).

```{r}
expect_equal(Mi, H%*%Mi)
```

Project the y vector onto each column of the X matrix and test if the sum of these projections is the same as yhat.

``{r}
prjt4 = ( x[,1] %*% t(x[,1]) / as.numeric( t(x[,1]) %*% x[,1]) ) %*% C
prjt5 = ( x[,2] %*% t(x[,2]) / as.numeric( t(x[,2]) %*% x[,2]) ) %*% C
prjt6 = ( x[,3] %*% t(x[,3]) / as.numeric( t(x[,3]) %*% x[,3]) ) %*% C
```
```

Convert this design matrix into Q, an orthonormal matrix.

```{r}
#Q = (diag(nrow(iris))-y) %*% C
#Q
```

Project the y vector onto each column of the Q matrix and test if the sum of these projections is the same as yhat.

```{r}
#prjt11 = ( x[,1] %*% t(x[,1]) / as.numeric( t(x[,1]) %*% x[,1]) ) %*% Q
#prjt22 = ( x[,2] %*% t(x[,2]) / as.numeric( t(x[,2]) %*% x[,2]) ) %*% Q
#prjt33 = ( x[,3] %*% t(x[,3]) / as.numeric( t(x[,3]) %*% x[,3]) ) %*% Q
```

Find the p=3 linear OLS estimates if Q is used as the design matrix using the `lm` method. Is the OLS solution the same as the OLS solution for X?

```{r}

```

Use the predict function and ensure that the predicted values are the same for both linear models: the one created with X  as its design matrix and the one created with Q as its design matrix.

```{r}
```


Clear the workspace and load the boston housing data and extract X and y. The dimensions are n = 506 and p = 13. Create a matrix that is (p + 1) x (p + 1) full of NA's. Label the columns the same columns as X. Do not label the rows. For the first row, find the OLS estimate of the y regressed on the first column only and put that in the first entry. For the second row, find the OLS estimates of the y regressed on the first and second columns of X only and put them in the first and second entries. For the third row, find the OLS estimates of the y regressed on the first, second and third columns of X only and put them in the first, second and third entries, etc. For the last row, fill it with the full OLS estimates.

```{r}
#TO-DO
```

Why are the estimates changing from row to row as you add in more predictors?

#TO-DO

Create a vector of length p+1 and compute the R^2 values for each of the above models. 

```{r}
#TO-DO
```

Is R^2 monotonically increasing? Why?

#TO-DO